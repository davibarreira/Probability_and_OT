\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{amsfonts, amsmath, amsthm, amssymb}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{bigints}
\usepackage{glossaries}
\usepackage{graphicx}
% \graphicspath{ {./images/} }

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\newcounter{countCode}
\lstnewenvironment{code} [1][caption=Ponme caption, label=default]{%
	\renewcommand*{\lstlistingname}{Listado} 
	\setcounter{lstlisting}{\value{countCode}} 
	\lstset{ %
	language=java,
	basicstyle=\ttfamily\footnotesize,       % the size of the fonts that are used for the code
	numbers=left,                   % where to put the line-numbers
	numberstyle=\sc,      % the size of the fonts that are used for the line-numbers
	stepnumber=1,                   % the step between two line-numbers. 
	numbersep=5pt,                 % how far the line-numbers are from the code
	numberstyle=\color{red!50!blue},
    	backgroundcolor=\color{lightgray!20},
	rulecolor=\color{blue},
	keywordstyle=\color{red}\bfseries,
	showspaces=false,               % show spaces adding particular underscores
	showstringspaces=false,         % underline spaces within strings
	showtabs=false,                 % show tabs within strings adding particular underscores
	frame=single,                   % adds a frame around the code
	framexleftmargin=0mm,
	numberblanklines=false,
	xleftmargin=5pt,
	breaklines=true,
	breakatwhitespace=true,
	breakautoindent=true,
	captionpos=t,
	texcl=true,
	tabsize=2,                      % sets default tabsize to 3 spaces
	extendedchars=true,
	inputencoding=utf8, 
	escapechar=\%,
	morekeywords={print, println, size, background, strokeWeight, fill, line, rect, ellipse, triangle, arc, save, PI, HALF_PI, QUARTER_PI, TAU, TWO_PI, width, height,},
	emph=[1]{print,println,}, emphstyle=[1]{\color{blue}}, % Mis palabras clave.
	emph=[2]{width,height,}, emphstyle=[2]{\bf\color{violet}}, % Mis palabras clave.
	emph=[3]{PI, HALF_PI, QUARTER_PI, TAU, TWO_PI}, emphstyle=[3]\color{orange!50!violet}, % Mis palabras clave.
	emph=[4]{line, rect, ellipse, triangle, arc,}, emphstyle=[4]\color{green!70!black}, % Mis palabras clave.
	%emph=[5]{size, background, strokeWeight, fill,}, emphstyle=[5]{\tt \color{red!30!blue}}, % Mis palabras clave.
	%emph={[2]sqrt,baset}, emphstyle={[2]\color{blue}}, % f(sqrt(2)), sqrt a nivel 2 se pondr√° azul
	#1}}{\addtocounter{countCode}{1}}



\title{Probability \& Optimal Transport}
\author{Davi Sales Barreira}
\date{\today}
\begin{document}
\maketitle \tableofcontents 


\begin{abstract}
The main goal of these notes is to present an introduction to the transport inequalities,
which consist of methods of using Optimal Transport Theory to obtain concentration
inequalities in high-dimensional probability. Along the way, as new concepts are presented,
some side-lining will be done, with the aim to explore some of these new concepts, before
diving back in the proof of the inequalities.
\end{abstract}

% \section*{Notation}
% \begin{itemize}
% 	\item $P(\mathcal X)$ - S
% 	Most of the content regarding transportation inequality is from
% \end{itemize}

\section{introduction}
Given two probability distributions $\mu,\nu$, there are many situations
where one is interested in defining a way of measuring the
distance between them. The Wasserstein distance is a
metric that arises from the idea of optimal transport, and which has being
gaining attention in Statistics and Machine Learning. One prominent example is the so
called Wasserstein Generative Adversarial Network (WGAN), which uses this metric to evaluate
how well the model generated distribution approximates the "real" distribution of the
data. As will be shown shortly, the Wasserstein metric has several advantages compared to
other metrics.

There are several ways of defining distances between two
probability measures. Let's assume that $\nu, \mu$ are defined on 
$(\Omega, \mathcal F)$ and that $\nu \ll \lambda$,
$\mu \ll \lambda$ , for $\lambda$ representing the Lebesgue measure. Below
we present some example of distances:

\begin{align*} 
\text{Total Variation} &: \quad || \mu - \nu ||_{TV} =
\sup_{A \in \mathcal F} |\mu (A) - \nu(A)| = 
\frac{1}{2}\int_\Omega \left | \frac{d\mu}{d\lambda} - \frac{d\nu}{d\lambda} \right
|d\lambda \\
\\
\text{Hellinger} &: \quad \sqrt{\bigintss_\Omega \left(\sqrt{\frac{d\mu}{d\lambda}} -
\sqrt{\frac{d\nu}{d\lambda}} \right )^2d\lambda} \\
\\
L_2 &: \quad \int_\Omega \left(\frac{d\mu}{d\lambda} - \frac{d\nu}{d\lambda}\right)^2
d\lambda \\
% \text{Relative Entropy} &: \quad
% H(\nu \mid \mu) =
% \left\{
%   \begin{array}{@{}ll@{}}
%     \int_\mathcal X \log(\frac{d\nu}{d\mu})d\nu, & \text{if}\ \nu \ll\mu \\
%     +\infty, & \text{otherwise}
%   \end{array}\right.
%   \\
%   \\
\end{align*}

As pointed out by \citet{wassermanStatisicalMethods2018} in his lecture notes,
although such distances are useful, there are drawbacks:
\begin{itemize}
	\item If one distribution is discrete and the other is continuous,  they cannot
	be compared. If $X \sim U(0,1)$ and $Y$ is uniform on $\{0,1/N, 2/N,...,1\}$,
	although this distributions are very similar, their total variation is 1 (which is
	the maximum value). The Wasserstein distance is $1/N$, which is reasonable.
	\item These distances ignore the "geometry of the underlying space", while the
	Wasserstein distance preserves it, as shown in Figure~\ref{fig:distances}.
	\item When "averaging" different distributions, one might be interested in obtaining
	a similar distribution, avoiding smoothing. This can be done using the Wasserstein
	barycenter, as shown in Figure~\ref{fig:barycenter}.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{images/Distributions_Distances.png}
    \caption{Each pair has the same distance in $L_2$, Hellinger and TV. But using
    Wasserstein, $X_1$ is closer to $X_2$ than to $X_3$.}
    \label{fig:distances}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=14cm]{images/Barycenter.png}
    \caption{In the left you have different distributions, and in the right,
    there is a comparison between averaging these distribution versus finding the
    Wasserstein barycenter.}
    \label{fig:barycenter}
\end{figure}

\section{Optimal Transport}

Let $\mathcal X$ be a polish space, and define a function
$c: \mathcal X \times \mathcal X \rightarrow [0,\infty)$, where $c$ is a lower semicontinuous function and $\mu,\nu \in P(\mathcal)$ (this means that $\mu$ and $\nu$ are
probability measures on $X$). Function $c$ is usually called the \textit{cost function}.
The Wasserstein distance arises from the Monge-Kantorovich optimal transport problem, which
seeks to find a map
$\pi: \mathcal X \times \mathcal X \rightarrow \mathcal X \times \mathcal X$, that
transport $\mu$ to $\nu$ with minimum cost.

\theoremstyle{definition}
\begin{definition}{Coupling}
A probability measure $\pi \in P(\mathcal X \times \mathcal Y)$ is called a coupling of
$\mu \in P(\mathcal X)$ and $\nu \in P(\mathcal Y)$ if it's marginal distributions
$\pi_1$ and $\pi_2$ are
$\mu$ and $\nu$.
\end{definition}

\begin{equation*}
	(MK) \quad \text{Minimize} \pi \in P(\mathcal X^2) \mapsto
	\int_{\mathcal X^2} c(x,y) d\pi(x,y) \ \text{subject to } \pi
	\text{coupling of } (\nu,\mu)
\end{equation*}

From solving the above problem, one obtains the optimal transport cost, given by:
\begin{equation}
	T_c(\nu, \mu) :=
	\inf \left\{
		\int_{\mathcal X^2} c(x,y) d\pi(x,y) \ ; \pi_1 = \nu, \pi_2 = \mu
	\right\}
\end{equation}



% section  (end)

% \section*{References}
% \addcontentsline{toc}{section}{References}
  \bibliography{probability_ot}
  \bibliographystyle{plainnat}
  % \bibliographystyle{plainnat}
  % \bibliographystyle{plain}
  % \bibliographystyle{abbrv}
\end{document}

